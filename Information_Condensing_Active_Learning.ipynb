{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Information Condensing Active Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO0l/ORuWDqc9/ipud4Mq1o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiahfong/incoherent-thoughts/blob/develop/Information_Condensing_Active_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBNqD0KJLGFT",
        "colab_type": "text"
      },
      "source": [
        "# Information Condensing Active Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ctc_LknFLJYR",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "Acquisition functions usually use model predictions/point locations (i.e. input features or learned representation space) to decide which points will improve model accuracy.\n",
        "> IDEA: does latent disentanglement + distance metric work well? Find latent representations with greatest distance according to some distance metric\n",
        "\n",
        "**GOAL**: develop and active learning acquisition function to select points that maximise the _eventual test accuracy_. This minimises the uncertainty on the unlabelled set.\n",
        "> Conspicuously did not mention the speed at which it reaches the desired test accuracy?\n",
        "\n",
        "* Selecting points that maximise MI between model params and labels doesn’t always minimise the model’s uncertainty on the unlabelled pool. Paper will demonstrate this point\n",
        "* Instead, ICAL searches for the batch that maximises statistical dependency between the model’s predictions on the batch (B) and the model’s predictions on the unlabelled pool (instead of model params).\n",
        "* Greedy algorithm employed to search for such a batch B (cf. feature selection)\n",
        "* HSIC instead of MI as MI is hard to approximate using just samples (Song & Ernon 2019)\n",
        "* HSIC is also differentiable, applicable in areas where MI would be difficult to make work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "074n-WY3LdI6",
        "colab_type": "text"
      },
      "source": [
        "# Related work\n",
        "\n",
        "## Bayesian active learning by disagreement (BALD)\n",
        "MI between unlabelled labels and model parameters. Turns out to be the points where the models are individually confident but are disagreeing with one another.\n",
        "\n",
        "## Guo & Schuurmans (2008)\n",
        "Find a batch such that the post-acquisition model is confident on the training set and has low uncertainty on the unlabelled dataset. This implies retraining the model for every candidate batch (read: exponential!) and is infeasible for large neural nets (method was mainly used for logistic regression)\n",
        "\n",
        "## BMDR\n",
        "Selects points that are close to the decision boundary while maintaining the overall sample distribution. The overall sample distribution is measured using maximum mean discrepancy (MMD) on *input features* between candidate batch and the set of all points (low MMD implies more representative batch). This method requires a decision boundary to work.\n",
        "\n",
        "## BMAL\n",
        "Selects a batch such that the Fisher information matrix between the batch and the\n",
        "unlabelled pool is as close as possible. Note, this matrix is quadratic in the number of parameters, thus infeasible for NNs.\n",
        "\n",
        "## Filtered Active Subset Selection\n",
        "1. Pick a subset of points $\\mathcal{B'}$ from the unlabelled pool where the model is most uncertain. Uncertainty is quantified using entropy in this case: $H(y|x, D_{train})$. Recall, entropy is low if the models are disagreeing with each other (first term in BALD).\n",
        "2. Select a subset of points in `1.` that are representative as a whole (similar to ICAL in this regard) which favours points that can represent diversity of the set in `1.`. In other words, find $\\mathcal{B}$ such that $f(\\mathcal{B}) = \\displaystyle\\sum_{y\\in\\mathcal{Y}}\\displaystyle\\sum_{i\\in V^y}\\text{max}_{s\\in \\mathcal{B} \\cap V^y}w(i, s)$ where:\n",
        "    - $V^y$ is the set of points in $\\mathcal{B'}$ that has predicted label $y$. ($V^y \\subseteq \\mathcal{B}'$)\n",
        "    - $w(i, s)$ is a similarity function (e.g. $w(i, s) = d - ||x_i - x_s||^2_2$ where $d$ is the maximum possible distance between any two points)\n",
        "\n",
        "\n",
        "## BatchBALD\n",
        "Extending BALD to account for acquisition size of > 1. Eliminates overcounting in BALD when candidate batch size is > 1.\n",
        "\n",
        "## Bayesian Batch Active Learning as Sparse Subset Approximation\n",
        "Adapts Bayesian Coreset approach. This approach changes the batch size for every acquisition.\n",
        "\n",
        "## DeepFool\n",
        "Uses the concept of advesarial examples to find points close to the decision boundary.\n",
        "The distance between an example and one of its adversarial examples is used as an approximation of its distance to the current decision boundary (presumably a proportional relationship?). This approach mitigates the need to have an explicit decision boundary.\n",
        "\n",
        "\n",
        "\n",
        "## Others\n",
        "1. Sener & Savarese (2017) and FF-Comp (Geifman & El-Yaniv, 2017) frames the problem as a core-set selection problem\n",
        "2. Discriminative Active Learning [DAL](https://openreview.net/pdf?id=rJl-HsR9KX) trains a classifier to distinguish between labelled and unlabelled. If the classifier is confident that the datapoint is unlabelled, then intuitively these points are most unlike the labelled points and thus should be informative.\n",
        "3. [BADGE](https://arxiv.org/pdf/1906.03671.pdf) samples points which are high in magnitude and diverse in a hallucinated gradient space w.r.t. the last layer of a NN.\n",
        "\n",
        "These methods are not model agnostic as the require access to learnt representational space.\n",
        "\n",
        "TODO: find out how these methods work (esp. about learnt representational spaces)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtvVGYSLZRYO",
        "colab_type": "text"
      },
      "source": [
        "# Background\n",
        "\n",
        "The paper defines *dependency* $\\delta$ between a set of random variables $X_{1:n}$ as follows:\n",
        "\n",
        "$$\n",
        "\\delta(X_{1:n}) = \\Lambda(P_{1:n}, \\otimes_iP_i)\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "1. $P_{1:n}$ is the joint distribution of $X_{1:n}$\n",
        "2. $P_i$ is the marginal distribution of $X_i$\n",
        "3. $\\otimes_i$ the product of marginals\n",
        "4. $\\Lambda$ a divergence function. If $\\Lambda$ is KL divergence, then we get MI as the dependency measure ($\\mathbb{I}[X, Y] = D_{KL}(P(x, y)\\ ||\\ P(x) P(y))$); if it was maximum mean discrepency (MMD), then the dependency measure is known as **Hilbert-Schmidt Independence Criterion** (HSIC).\n",
        "\n",
        "To extend two-variable HSIC to arbitiarily many variables, the authors used [_dHISC_](https://arxiv.org/pdf/1603.00285.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0X46uE2TGWzX",
        "colab_type": "text"
      },
      "source": [
        "# Motivation\n",
        "\n",
        "\n",
        "The upshot of this section is that picking the points with the most amount of information w.r.t. __model parameters__, for example BALD, could in fact increase the uncertainty of prediction on unlabelled data. The paper gave a toy example to demonstrate this and suggests picking points with the most amount of information w.r.t. model's predictions on the __unlabelled points__."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xq65CwaIK6vQ",
        "colab_type": "text"
      },
      "source": [
        "# Information Condensing Active Learning (ICAL)\n",
        "\n",
        "## Acquisition function\n",
        "\n",
        "The authors defined the acquisition function as follows:\n",
        "$$\n",
        "\\alpha_{ICAL}(\\{x_1, \\dots, x_B\\}, \\delta) = \\frac{1}{|\\mathcal{D}_U|}\\displaystyle\\sum_{x'\\in\\mathcal{D}_U} \\delta(y_{x'}, \\{y_{x_1}, \\dots, y_{x_B}\\})\n",
        "$$\n",
        "\n",
        "where $\\mathcal{D}_U$ is the unlabelled pool and $\\delta$ the dependency measure. (see above). \n",
        "\n",
        "TODO: figure out the conflicting use of notation: $\\delta(X_{1:n})$ vs $\\delta(y_{x'}, \\{y_{x_1}, \\dots, y_{x_B}\\})$\n",
        "\n",
        "\n",
        "\n",
        "## Scaling ICAL\n",
        "\n",
        "There are two types of ICAL introduced: normal ICAL and ICAL-pointwise.\n",
        "ICAL-pointwise turns out to be more time efficient and peformed slightly better in the early acquisitions.\n",
        "\n",
        "## Larger batch sizes\n",
        "\n",
        "The authors are aware that by scaling to larger batch sizes, they might sacrifice diversity in the batch.\n",
        "\n",
        "> ICAL-pointwise can accommodate larger batch sizes compared to normal ICAL\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOVORDS3O07I",
        "colab_type": "text"
      },
      "source": [
        "# Results\n",
        "\n",
        "The authors benchmarked a subset of the methods outlined in _Related work_, namely: BatchBALD, BALD, random acquisition, ICAL, BayesCoreset, FASS, and Max entropy.\n",
        "\n",
        "## MNIST & Repeated MNIST\n",
        "\n",
        "The results from ICAL, BatchBALD, and BayesCoreset are neck and neck.\n",
        "(30 acquisitions; batch acquisition size = 10; 50 MC dropout samples)\n",
        "\n",
        "## EMNIST\n",
        "ICAL outperforms BatchBALD and BayesCoreset. \n",
        "(60 acquisitions; batch acquisition size = 5; 50 MC dropout samples)\n",
        "\n",
        "The authors attributed ICAL's performance to its ability to acquire a more diverse and balanced set of batches while stating that the other methods have under and over represented classes. The figure (Figure 4) shows that BatchBALD, random acquisition, and Max Entropy completely missed some classes in `EMNIST` (47 classes)! (Probably a contrived example given random acquisition should on average have ~6 points per class).\n",
        "\n",
        "The authors argues that ICAL is more robust even as the number of classes increases (47) whereas the other alternatives degenerate.\n",
        "\n",
        "## Fashion MNIST\n",
        "Likewise, ICAL outperforms the BatchBALD and BayesCoreset. **Interestingly, the runner-up for FashionMNIST is random acquisition, besting both BatchBALD and BayesCoreset.**\n",
        "\n",
        "(30 acquisitions; batch acquisition size = 10; 100 MC dropout samples)\n",
        "\n",
        "## CIFAR-10 & CIFAR-100\n",
        "ICAL outperforms all others. BayesCoreset performs the closest to ICAL.\n",
        "\n",
        "(10 acquisitions on CIFAR-10 and 7 acquisitions on CIFAR-100; batch acquisition size = 3,000; MC dropout samples _not mentioned_).\n",
        "\n",
        "> BatchBALD ran out of memory on both datasets and was not benchmarked.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gQf1qi9Uobx",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "ICAL is model agnostic, applicable to both classification and regression tasks, and scales well with large batch sizes and large unlabelled pool sizes.\n",
        "\n",
        "Future work includes even larger batch sizes (exploring techniques used in feature selection) and a hybird of getting the most information for __both__ model parameters (like BALD) and labels of the unlabelled pool in a single acquisition function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDaIhaSYVTw1",
        "colab_type": "text"
      },
      "source": [
        "# Closing remarks (Relatability to AL project)\n",
        "\n",
        "1. The methods in _related work_ is worth exploring as there are some interesting ideas about AL in the context of deep learning and the authors did not benchmark all of them.\n",
        "2. Extend BatchBALD to work with larger batch sizes (presumably 3000 samples at once is much)\n",
        "3. Explore the direction of _diversity_ in the samples acquired. Does diversity decrease as the number of classes increase, as the authors postulated?\n",
        "4. Does dHISC really make a difference? What if we used MI as the dependecy measure $\\delta$ instead of dHISC? (The authors cited papers that support their claim (Song & Ermon 2019)) It will no longer be ICAL but we could try taking the MI w.r.t. labels of the unlabelled pool as opposed to the usual model parameters.\n",
        "5. The idea of a hybrid acquisition function seems appealing; can we combine information gained from labels of the unlabelled pool and model parameters together? Naively one could consider a weighted approach:\n",
        "    - $\\alpha$ * MI w.r.t. model params + $\\beta$ * MI w.r.t. labels of unlabelled pool or\n",
        "    - $\\alpha$ * MI w.r.t. model params + $\\beta$ * dHSIC w.r.t. labels of unlabelled pool or\n",
        "    - $\\alpha$ * dHISC w.r.t. model params + $\\beta$ * MI w.r.t. labels of unlabelled pool or\n",
        "    - $\\alpha$ * dHISC w.r.t. model params + $\\beta$ * dHISC w.r.t. labels of unlabelled pool for some $\\alpha, \\beta \\in \\mathbb{R}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNj395eTXTI1",
        "colab_type": "text"
      },
      "source": [
        "# Compilation of TODOs\n",
        "\n",
        "1. Find out how the methods in _Related work_ work (esp. about learnt representational spaces).\n",
        "2. Figure out the conflicting use of notation: $\\delta(X_{1:n})$ vs $\\delta(y_{x'}, \\{y_{x_1}, \\dots, y_{x_B}\\})$.\n",
        "3. (Understand and) Implement [dHSIC](https://arxiv.org/pdf/1603.00285.pdf)/ICAL.\n",
        "4. Read [Song & Ermon (2019)](https://arxiv.org/pdf/1910.06222.pdf)"
      ]
    }
  ]
}