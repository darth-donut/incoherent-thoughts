{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Information Condensing Active Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMAIg/y8oH3J51p3meFBdgU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiahfong/incoherent-thoughts/blob/develop/Information_Condensing_Active_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBNqD0KJLGFT",
        "colab_type": "text"
      },
      "source": [
        "# Information Condensing Active Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ctc_LknFLJYR",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "Acquisition functions usually use model predictions/point locations (i.e. input features or learned representation space) to decide which points will improve model accuracy.\n",
        "> IDEA: does latent disentanglement + distance metric work well? Find latent representations with greatest distance according to some distance metric\n",
        "\n",
        "**GOAL**: develop and active learning acquisition function to select points that maximise the _eventual test accuracy_. This minimises the uncertainty on the unlabelled set.\n",
        "> Conspicuously did not mention the speed at which it reaches the desired test accuracy?\n",
        "\n",
        "* Selecting points that maximise MI between model params and labels doesn’t always minimise the model’s uncertainty on the unlabelled pool. Paper will demonstrate this point\n",
        "* Instead, ICAL searches for the batch that maximises statistical dependency between the model’s predictions on the batch (B) and the model’s predictions on the unlabelled pool (instead of model params).\n",
        "* Greedy algorithm employed to search for such a batch B (cf. feature selection)\n",
        "* HSIC instead of MI as MI is hard to approximate using just samples (Song & Ernon 2019)\n",
        "* HSIC is also differentiable, applicable in areas where MI would be difficult to make work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "074n-WY3LdI6",
        "colab_type": "text"
      },
      "source": [
        "# Related work\n",
        "\n",
        "## Bayesian active learning by disagreement (BALD)\n",
        "MI between unlabelled labels and model parameters. Turns out to be the points where the models are individually confident but are disagreeing with one another.\n",
        "\n",
        "## Guo & Schuurmans (2008)\n",
        "Find a batch such that the post-acquisition model is confident on the training set and has low uncertainty on the unlabelled dataset. This implies retraining the model for every candidate batch (read: exponential!) and is infeasible for large neural nets (method was mainly used for logistic regression)\n",
        "\n",
        "## BMDR\n",
        "Selects points that are close to the decision boundary while maintaining the overall sample distribution. The overall sample distribution is measured using maximum mean discrepancy (MMD) on *input features* between candidate batch and the set of all points (low MMD implies more representative batch). This method requires a decision boundary to work.\n",
        "\n",
        "## BMAL\n",
        "Selects a batch such that the Fisher information matrix between the batch and the\n",
        "unlabelled pool is as close as possible. Note, this matrix is quadratic in the number of parameters, thus infeasible for NNs.\n",
        "\n",
        "## Filtered Active Subset Selection\n",
        "1. Pick a subset of points $\\mathcal{B'}$ from the unlabelled pool where the model is most uncertain. Uncertainty is quantified using entropy in this case: $H(y|x, D_{train})$. Recall, entropy is low if the models are disagreeing with each other (first term in BALD).\n",
        "2. Select a subset of points in `1.` that are representative as a whole (similar to ICAL in this regard) which favours points that can represent diversity of the set in `1.`. In other words, find $\\mathcal{B}$ such that $f(\\mathcal{B}) = \\displaystyle\\sum_{y\\in\\mathcal{Y}}\\displaystyle\\sum_{i\\in V^y}\\text{max}_{s\\in \\mathcal{B} \\cap V^y}w(i, s)$ where:\n",
        "    - $V^y$ is the set of points in $\\mathcal{B'}$ that has predicted label $y$. ($V^y \\subseteq \\mathcal{B}'$)\n",
        "    - $w(i, s)$ is a similarity function (e.g. $w(i, s) = d - ||x_i - x_s||^2_2$ where $d$ is the maximum possible distance between any two points)\n",
        "\n",
        "\n",
        "## BatchBALD\n",
        "Extending BALD to account for acquisition size of > 1. Eliminates overcounting in BALD when candidate batch size is > 1.\n",
        "\n",
        "## Bayesian Batch Active Learning as Sparse Subset Approximation\n",
        "Adapts Bayesian Coreset approach. This approach changes the batch size for every acquisition.\n",
        "\n",
        "## DeepFool\n",
        "Uses the concept of advesarial examples to find points close to the decision boundary.\n",
        "The distance between an example and one of its adversarial examples is used as an approximation of its distance to the current decision boundary (presumably a proportional relationship?). This approach mitigates the need to have an explicit decision boundary.\n",
        "\n",
        "\n",
        "\n",
        "## Others\n",
        "1. Sener & Savarese (2017) and FF-Comp (Geifman & El-Yaniv, 2017) frames the problem as a core-set selection problem\n",
        "2. Discriminative Active Learning [DAL](https://openreview.net/pdf?id=rJl-HsR9KX) trains a classifier to distinguish between labelled and unlabelled. If the classifier is confident that the datapoint is unlabelled, then intuitively these points are most unlike the labelled points and thus should be informative.\n",
        "3. [BADGE](https://arxiv.org/pdf/1906.03671.pdf) samples points which are high in magnitude and diverse in a hallucinated gradient space w.r.t. the last layer of a NN.\n",
        "\n",
        "These methods are not model agnostic as the require access to learnt representational space.\n",
        "\n",
        "TODO: find out how these methods work (esp. about learnt representational spaces)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtvVGYSLZRYO",
        "colab_type": "text"
      },
      "source": [
        "# Background\n",
        "\n",
        "The paper defines *dependency* $\\delta$ between a set of random variables $X_{1:n}$ as follows:\n",
        "\n",
        "$$\n",
        "\\delta(X_{1:n}) = \\Lambda(P_{1:n}, \\otimes_iP_i)\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "1. $P_{1:n}$ is the joint distribution of $X_{1:n}$\n",
        "2. $\\otimes_i P_i$ the product of marginals\n",
        "3. $\\Lambda$ a divergence function. If $\\Lambda$ is KL divergence, then we get MI as the dependency measure ($\\mathbb{I}[X, Y] = D_{KL}(P(x, y)\\ ||\\ P(x) P(y))$); if it was maximum mean discrepency (MMD), then the dependency measure is known as **Hilbert-Schmidt Independence Criterion** (HSIC).\n",
        "\n",
        "To extend two-variable HSIC to arbitiarily many variables, the authors used [_dHISC_](https://arxiv.org/pdf/1603.00285.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0X46uE2TGWzX",
        "colab_type": "text"
      },
      "source": [
        "# Motivation\n",
        "\n",
        "\n",
        "The upshot of this section is that picking the points with the most amount of information w.r.t. __model parameters__, for example BALD, could in fact increase the uncertainty of prediction on unlabelled data. The paper gave a toy example to demonstrate this and suggests picking points with the most amount of information w.r.t. model's predictions on the __unlabelled points__."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xq65CwaIK6vQ",
        "colab_type": "text"
      },
      "source": [
        "# Information Condensing Active Learning (ICAL)\n",
        "\n",
        "## Acquisition function\n",
        "\n",
        "The authors defined the acquisition function as follows:\n",
        "$$\n",
        "\\alpha_{ICAL}(\\{x_1, \\dots, x_B\\}, \\delta) = \\frac{1}{|\\mathcal{D}_U|}\\displaystyle\\sum_{x'\\in\\mathcal{D}_U} \\delta(y_{x'}, \\{y_{x_1}, \\dots, y_{x_B}\\})\n",
        "$$\n",
        "\n",
        "where $\\mathcal{D}_U$ is the unlabelled pool and $\\delta$ the dependency measure. (see above). \n",
        "\n",
        "\n",
        "\n",
        "## 1. Scaling ICAL\n",
        "\n",
        "There are two types of ICAL introduced: normal ICAL and ICAL-pointwise. Theorem 2 showed that we could simplify the acquistion function above to:\n",
        "\n",
        "$$\\frac{1}{|\\mathcal{D}_U|}\\displaystyle\\sum_{x'\\in\\mathcal{D}_U} dHSIC(k^{x'}, \\{k^{x_1}, \\dots, k^{x_B}\\}) = \\frac{1}{|\\mathcal{D}_U|} dHSIC(\\displaystyle\\sum_{x'\\in\\mathcal{D}_U} k^{x'}, \\{k^{x_1}, \\dots, k^{x_B}\\})\n",
        "$$\n",
        "\n",
        "However, if the number of MC dropout samples $m$ is $<2D$ where $D$ is the number of variables in _d_HSIC, then _d_HSIC = 0 (see [Pfister et al.](https://arxiv.org/pdf/1603.00285.pd)). Hence, as B grows, there is a need to increase the number of MC dropout samples $m$. The paper suggests 2 strategies to circumvent this whilst still maintaining batch diversity: normal ICAL and ICAL-pointwise.\n",
        "\n",
        "### 1.1 Normal ICAL\n",
        "The kernel matrices of the candidate batch, $k^{x_1}, \\dots, k^{x_B}$, are averaged. Furthermore, instead of taking the sum of kernels over the entire $\\mathcal{D}_U$, $r$ points are subsampled from $\\mathcal{D}_U$ instead. In all their experiments, $r = |\\mathcal{R}| = 200$. In other words:\n",
        "\n",
        "$$\\frac{1}{|\\mathcal{R}|} dHSIC(\\displaystyle\\sum_{x'\\in\\mathcal{R}} k^{x'}, \\frac{1}{B} \\displaystyle\\sum_{i = 1}^{B} k^{x_i})$$\n",
        "\n",
        "Thus, dHISC essentially reduces to $d = 2$, i.e. a regular 2-variable HSIC.\n",
        "> It's worth noting that the main motivation behind using $\\mathcal{R}$ instead of the more representative $\\mathcal{D}_U$ is *perhaps* to introduce some noise in dHSIC estimator; all $|\\mathcal{D}_U|$ kernels have to be computed when greedily searching for the next point to acquire anyway -- there isn't any obvious (significant) computational savings in just using $|\\mathcal{R}|$ points!\n",
        "\n",
        "### 1.2 ICAL-pointwise\n",
        "TODO\n",
        "\n",
        "### 1.3 Larger batch sizes\n",
        "Instead of acquiring one point at a time, the authors suggested acquiring L points at a time, hence a full acquisition of $B$ points only requires $\\frac{B}{L}$ iterations. The authors are aware that they might sacrifice diversity in the batch by doing so."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOVORDS3O07I",
        "colab_type": "text"
      },
      "source": [
        "# Results\n",
        "\n",
        "The authors benchmarked a subset of the methods outlined in _Related work_, namely: BatchBALD, BALD, random acquisition, ICAL, BayesCoreset, FASS, and Max entropy.\n",
        "\n",
        "## MNIST & Repeated MNIST\n",
        "\n",
        "The results from ICAL, BatchBALD, and BayesCoreset are neck and neck.\n",
        "(30 acquisitions; batch acquisition size = 10; 50 MC dropout samples)\n",
        "\n",
        "## EMNIST\n",
        "ICAL outperforms BatchBALD and BayesCoreset. \n",
        "(60 acquisitions; batch acquisition size = 5; 50 MC dropout samples)\n",
        "\n",
        "The authors attributed ICAL's performance to its ability to acquire a more diverse and balanced set of batches while stating that the other methods have under and over represented classes. The figure (Figure 4) shows that BatchBALD, random acquisition, and Max Entropy completely missed some classes in `EMNIST` (47 classes)! (Probably a contrived example given random acquisition should on average have ~6 points per class).\n",
        "\n",
        "The authors argues that ICAL is more robust even as the number of classes increases (47) whereas the other alternatives degenerate.\n",
        "\n",
        "## Fashion MNIST\n",
        "Likewise, ICAL outperforms the BatchBALD and BayesCoreset. **Interestingly, the runner-up for FashionMNIST is random acquisition, besting both BatchBALD and BayesCoreset.**\n",
        "\n",
        "(30 acquisitions; batch acquisition size = 10; 100 MC dropout samples)\n",
        "\n",
        "## CIFAR-10 & CIFAR-100\n",
        "ICAL outperforms all others. BayesCoreset performs the closest to ICAL.\n",
        "\n",
        "(10 acquisitions on CIFAR-10 and 7 acquisitions on CIFAR-100; batch acquisition size = 3,000; MC dropout samples _not mentioned_).\n",
        "\n",
        "> BatchBALD ran out of memory on both datasets and was not benchmarked.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gQf1qi9Uobx",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "ICAL is model agnostic, applicable to both classification and regression tasks, and scales well with large batch sizes and large unlabelled pool sizes.\n",
        "\n",
        "Future work includes even larger batch sizes (exploring techniques used in feature selection) and a hybird of getting the most information for __both__ model parameters (like BALD) and labels of the unlabelled pool in a single acquisition function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDaIhaSYVTw1",
        "colab_type": "text"
      },
      "source": [
        "# Closing remarks (Relatability to AL project)\n",
        "\n",
        "1. The methods in _related work_ is worth exploring as there are some interesting ideas about AL in the context of deep learning and the authors did not benchmark all of them.\n",
        "2. Extend BatchBALD to work with larger batch sizes (presumably 3000 samples at once is much)\n",
        "3. Explore the direction of _diversity_ in the samples acquired. Does diversity decrease as the number of classes increase, as the authors postulated?\n",
        "4. Does dHISC really make a difference? What if we used MI as the dependecy measure $\\delta$ instead of dHISC? (The authors cited papers that support their claim (Song & Ermon 2019)) It will no longer be ICAL but we could try taking the MI w.r.t. labels of the unlabelled pool as opposed to the usual model parameters.\n",
        "5. The idea of a hybrid acquisition function seems appealing; can we combine information gained from labels of the unlabelled pool and model parameters together? Naively one could consider a weighted approach:\n",
        "    - $\\alpha$ * MI w.r.t. model params + $\\beta$ * MI w.r.t. labels of unlabelled pool or\n",
        "    - $\\alpha$ * MI w.r.t. model params + $\\beta$ * dHSIC w.r.t. labels of unlabelled pool or\n",
        "    - $\\alpha$ * dHISC w.r.t. model params + $\\beta$ * MI w.r.t. labels of unlabelled pool or\n",
        "    - $\\alpha$ * dHISC w.r.t. model params + $\\beta$ * dHISC w.r.t. labels of unlabelled pool for some $\\alpha, \\beta \\in \\mathbb{R}$\n",
        "6. Reconsider adding noise to the estimator -- is subsampling $\\mathcal{R}$ a benificial action? Can this be applied in other acquisition functions? Will they improve?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNj395eTXTI1",
        "colab_type": "text"
      },
      "source": [
        "# Compilation of TODOs\n",
        "\n",
        "1. Find out how the methods in _Related work_ work (esp. about learnt representational spaces).\n",
        "2. Read [Song & Ermon (2019)](https://arxiv.org/pdf/1910.06222.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4k0qoe_mTrTI",
        "colab_type": "code",
        "outputId": "6ecde26a-2434-43b9-bfe8-0b0b77491eef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "!pip install --no-cache-dir git+https://github.com/jiahfong/alr.git@b27090b74fdaa165a44f40ec35b9b2050fca0b71"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/jiahfong/alr.git@b27090b74fdaa165a44f40ec35b9b2050fca0b71\n",
            "  Cloning https://github.com/jiahfong/alr.git (to revision b27090b74fdaa165a44f40ec35b9b2050fca0b71) to /tmp/pip-req-build-ofujltdc\n",
            "  Running command git clone -q https://github.com/jiahfong/alr.git /tmp/pip-req-build-ofujltdc\n",
            "  Running command git checkout -q b27090b74fdaa165a44f40ec35b9b2050fca0b71\n",
            "Requirement already satisfied (use --upgrade to upgrade): alr==0.0.0b1 from git+https://github.com/jiahfong/alr.git@b27090b74fdaa165a44f40ec35b9b2050fca0b71 in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from alr==0.0.0b1) (1.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from alr==0.0.0b1) (4.38.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from alr==0.0.0b1) (1.18.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from alr==0.0.0b1) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->alr==0.0.0b1) (0.14.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->alr==0.0.0b1) (1.4.1)\n",
            "Building wheels for collected packages: alr\n",
            "  Building wheel for alr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for alr: filename=alr-0.0.0b1-cp36-none-any.whl size=10666 sha256=42a9b6e4dc2928cb28f8592eb217d11a03b16fbc1ad2f907e59ba2405262404a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jj91mj6_/wheels/4c/c9/61/4b7a8ab8f93e92166c0115339c8cd7bd79a9f29650f3f3ea69\n",
            "Successfully built alr\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCXrYunV03IP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import typing\n",
        "\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from alr import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mg29Sw0fJDWq",
        "colab_type": "code",
        "outputId": "d4521e83-045b-449e-ccb5-ccd94378b5f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "transforms = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor()\n",
        "])\n",
        "train = torchvision.datasets.MNIST('./data', download=True, train=True, transform=transforms)\n",
        "test = torchvision.datasets.MNIST('./data', download=True, train=False, transform=transforms)\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbHXAhIOjREM",
        "colab_type": "code",
        "outputId": "365077ea-0da9-48f7-d7dc-ddc5b7fb7612",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "X_train, y_train = train.data.unsqueeze(1) / 255.0, train.targets\n",
        "X_test, y_test = test.data.unsqueeze(1) / 255.0, test.targets\n",
        "\n",
        "print(X_train.size(), y_train.size())\n",
        "print(X_test.size(), y_test.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([60000, 1, 28, 28]) torch.Size([60000])\n",
            "torch.Size([10000, 1, 28, 28]) torch.Size([10000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PJS2dehj7DP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dl_params = dict(pin_memory=True, num_workers=2)\n",
        "INITIAL_EPOCHS = EPOCHS = 100\n",
        "INIT_TRAIN_SIZE = 20\n",
        "NFORWARD = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoJ41_qvlCRR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # architecture of this model came from:\n",
        "        # https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
        "        self.maxpool = nn.MaxPool2d(2)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(12*12*64, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.maxpool(x)\n",
        "        # NOTE: this is NOT Dropout2D!\n",
        "        x = F.dropout(x, p=.25, training=True)\n",
        "        x = self.flatten(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, p=.5, training=True)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = MCDropout(Net(), NFORWARD).to(device)\n",
        "optimiser = torch.optim.Adam(model.parameters())\n",
        "test_data_loader = torch.utils.data.DataLoader(ALRDataset(X_test, y_test),\n",
        "                                               batch_size=256, **dl_params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjYfeiNGlK7h",
        "colab_type": "code",
        "outputId": "0792347e-9b72-45a4-a5a4-5e774e3e9b35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "X_train, y_train, X_pool, y_pool = stratified_partition(X_train, y_train, train_size=INIT_TRAIN_SIZE)\n",
        "subset_pool = np.random.choice(X_pool.size(0), size=10_000, replace=False)\n",
        "X_pool, y_pool = X_pool[subset_pool], y_pool[subset_pool]\n",
        "print(X_train.size(), y_train.size())\n",
        "print(X_pool.size(), y_pool.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([20, 1, 28, 28]) torch.Size([20])\n",
            "torch.Size([10000, 1, 28, 28]) torch.Size([10000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Bo4KFAgkZ-3",
        "colab_type": "code",
        "outputId": "574c2c43-7a39-499b-9bca-26575fa10bb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "acquisition_function = ICAL(model, device=device, batch_size=256, **dl_params)\n",
        "history = run_experiment(model, acquisition_function,\n",
        "                         X_train, y_train, X_pool, y_pool,\n",
        "                         test_data_loader, optimiser, b=10,\n",
        "                         iters=30, init_epochs=INITIAL_EPOCHS, epochs=EPOCHS,\n",
        "                         device=device, batch_size=128, **dl_params)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Commencing initial training with 20 points\n",
            "100%|██████████| 100/100 [00:11<00:00,  8.74it/s, loss=0.00178]\n",
            "Accuracy = 0.5698\n",
            "=====\n",
            "Acquisition iteration 1 (3.33%), training size: 30\n",
            "100%|██████████| 100/100 [00:16<00:00,  6.04it/s, loss=0.108]\n",
            "Accuracy = 0.6155\n",
            "=====\n",
            "Acquisition iteration 2 (6.67%), training size: 40\n",
            "100%|██████████| 100/100 [00:16<00:00,  6.04it/s, loss=0.0494]\n",
            "Accuracy = 0.6447\n",
            "=====\n",
            "Acquisition iteration 3 (10.00%), training size: 50\n",
            "100%|██████████| 100/100 [00:16<00:00,  6.02it/s, loss=0.0291]\n",
            "Accuracy = 0.6766\n",
            "=====\n",
            "Acquisition iteration 4 (13.33%), training size: 60\n",
            "100%|██████████| 100/100 [00:16<00:00,  6.02it/s, loss=0.0239]\n",
            "Accuracy = 0.7166\n",
            "=====\n",
            "Acquisition iteration 5 (16.67%), training size: 70\n",
            "100%|██████████| 100/100 [00:16<00:00,  5.97it/s, loss=0.057]\n",
            "Accuracy = 0.7554\n",
            "=====\n",
            "Acquisition iteration 6 (20.00%), training size: 80\n",
            "100%|██████████| 100/100 [00:16<00:00,  5.97it/s, loss=0.00781]\n",
            "Accuracy = 0.7872\n",
            "=====\n",
            "Acquisition iteration 7 (23.33%), training size: 90\n",
            "100%|██████████| 100/100 [00:16<00:00,  5.98it/s, loss=0.0265]\n",
            "Accuracy = 0.8031\n",
            "=====\n",
            "Acquisition iteration 8 (26.67%), training size: 100\n",
            "100%|██████████| 100/100 [00:16<00:00,  5.97it/s, loss=0.0238]\n",
            "Accuracy = 0.8202\n",
            "=====\n",
            "Acquisition iteration 9 (30.00%), training size: 110\n",
            "100%|██████████| 100/100 [00:16<00:00,  6.04it/s, loss=0.0219]\n",
            "Accuracy = 0.8263\n",
            "=====\n",
            "Acquisition iteration 10 (33.33%), training size: 120\n",
            "100%|██████████| 100/100 [00:16<00:00,  5.91it/s, loss=0.0452]\n",
            "Accuracy = 0.8273\n",
            "=====\n",
            "Acquisition iteration 11 (36.67%), training size: 130\n",
            "100%|██████████| 100/100 [00:16<00:00,  5.99it/s, loss=0.0432]\n",
            "Accuracy = 0.8625\n",
            "=====\n",
            "Acquisition iteration 12 (40.00%), training size: 140\n",
            "100%|██████████| 100/100 [00:16<00:00,  6.20it/s, loss=0.0745]\n",
            "Accuracy = 0.8443\n",
            "=====\n",
            "Acquisition iteration 13 (43.33%), training size: 150\n",
            "100%|██████████| 100/100 [00:17<00:00,  5.58it/s, loss=0.0814]\n",
            "Accuracy = 0.846\n",
            "=====\n",
            "Acquisition iteration 14 (46.67%), training size: 160\n",
            "100%|██████████| 100/100 [00:17<00:00,  5.64it/s, loss=0.0998]\n",
            "Accuracy = 0.8554\n",
            "=====\n",
            "Acquisition iteration 15 (50.00%), training size: 170\n",
            "100%|██████████| 100/100 [00:17<00:00,  5.63it/s, loss=0.0492]\n",
            "Accuracy = 0.858\n",
            "=====\n",
            "Acquisition iteration 16 (53.33%), training size: 180\n",
            "100%|██████████| 100/100 [00:17<00:00,  5.64it/s, loss=0.0948]\n",
            "Accuracy = 0.8627\n",
            "=====\n",
            "Acquisition iteration 17 (56.67%), training size: 190\n",
            "100%|██████████| 100/100 [00:17<00:00,  5.62it/s, loss=0.0634]\n",
            "Accuracy = 0.8735\n",
            "=====\n",
            "Acquisition iteration 18 (60.00%), training size: 200\n",
            "100%|██████████| 100/100 [00:16<00:00,  6.20it/s, loss=0.0887]\n",
            "Accuracy = 0.888\n",
            "=====\n",
            "Acquisition iteration 19 (63.33%), training size: 210\n",
            "100%|██████████| 100/100 [00:16<00:00,  6.18it/s, loss=0.0539]\n",
            "Accuracy = 0.8991\n",
            "=====\n",
            "Acquisition iteration 20 (66.67%), training size: 220\n",
            "100%|██████████| 100/100 [00:17<00:00,  5.63it/s, loss=0.035]\n",
            "Accuracy = 0.888\n",
            "=====\n",
            "Acquisition iteration 21 (70.00%), training size: 230\n",
            "100%|██████████| 100/100 [00:17<00:00,  5.66it/s, loss=0.0421]\n",
            "Accuracy = 0.9021\n",
            "=====\n",
            "Acquisition iteration 22 (73.33%), training size: 240\n",
            "100%|██████████| 100/100 [00:17<00:00,  5.59it/s, loss=0.0333]\n",
            "Accuracy = 0.905\n",
            "=====\n",
            "Acquisition iteration 23 (76.67%), training size: 250\n",
            "100%|██████████| 100/100 [00:17<00:00,  5.60it/s, loss=0.0338]\n",
            "Accuracy = 0.9086\n",
            "=====\n",
            "Acquisition iteration 24 (80.00%), training size: 260\n",
            "100%|██████████| 100/100 [00:18<00:00,  5.38it/s, loss=0.0957]\n",
            "Accuracy = 0.9251\n",
            "=====\n",
            "Acquisition iteration 25 (83.33%), training size: 270\n",
            "100%|██████████| 100/100 [00:18<00:00,  5.37it/s, loss=0.0502]\n",
            "Accuracy = 0.9348\n",
            "=====\n",
            "Acquisition iteration 26 (86.67%), training size: 280\n",
            "100%|██████████| 100/100 [00:18<00:00,  5.36it/s, loss=0.076]\n",
            "Accuracy = 0.9252\n",
            "=====\n",
            "Acquisition iteration 27 (90.00%), training size: 290\n",
            "100%|██████████| 100/100 [00:18<00:00,  5.36it/s, loss=0.125]\n",
            "Accuracy = 0.9246\n",
            "=====\n",
            "Acquisition iteration 28 (93.33%), training size: 300\n",
            "100%|██████████| 100/100 [00:18<00:00,  5.36it/s, loss=0.0396]\n",
            "Accuracy = 0.9229\n",
            "=====\n",
            "Acquisition iteration 29 (96.67%), training size: 310\n",
            "100%|██████████| 100/100 [00:18<00:00,  5.37it/s, loss=0.062]\n",
            "Accuracy = 0.9222\n",
            "=====\n",
            "Acquisition iteration 30 (100.00%), training size: 320\n",
            "100%|██████████| 100/100 [00:18<00:00,  5.37it/s, loss=0.0499]\n",
            "Accuracy = 0.9257\n",
            "=====\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKAs7dJaReIY",
        "colab_type": "code",
        "outputId": "e74ec3f2-3b9d-4c56-bfc9-609258938c7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# this time, try random acqusition\n",
        "model.reset_weights()\n",
        "ra_history = run_experiment(model, RandomAcquisition(),\n",
        "                            X_train, y_train, X_pool, y_pool,\n",
        "                            test_data_loader, optimiser, b=10,\n",
        "                            iters=30, init_epochs=INITIAL_EPOCHS, epochs=EPOCHS,\n",
        "                            device=device, batch_size=128, **dl_params)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Commencing initial training with 20 points\n",
            "100%|██████████| 100/100 [00:16<00:00,  5.99it/s, loss=0.0272]\n",
            "Accuracy = 0.5411\n",
            "=====\n",
            "Acquisition iteration 1 (3.33%), training size: 30\n",
            "100%|██████████| 100/100 [00:16<00:00,  5.97it/s, loss=0.00146]\n",
            "Accuracy = 0.5824\n",
            "=====\n",
            "Acquisition iteration 2 (6.67%), training size: 40\n",
            "100%|██████████| 100/100 [00:16<00:00,  5.95it/s, loss=0.0105]\n",
            "Accuracy = 0.6597\n",
            "=====\n",
            "Acquisition iteration 3 (10.00%), training size: 50\n",
            "100%|██████████| 100/100 [00:16<00:00,  5.92it/s, loss=0.0551]\n",
            "Accuracy = 0.6851\n",
            "=====\n",
            "Acquisition iteration 4 (13.33%), training size: 60\n",
            "100%|██████████| 100/100 [00:16<00:00,  5.95it/s, loss=0.00405]\n",
            "Accuracy = 0.7067\n",
            "=====\n",
            "Acquisition iteration 5 (16.67%), training size: 70\n",
            "100%|██████████| 100/100 [00:16<00:00,  5.89it/s, loss=0.0169]\n",
            "Accuracy = 0.7555\n",
            "=====\n",
            "Acquisition iteration 6 (20.00%), training size: 80\n",
            "100%|██████████| 100/100 [00:16<00:00,  5.93it/s, loss=0.00465]\n",
            "Accuracy = 0.7917\n",
            "=====\n",
            "Acquisition iteration 7 (23.33%), training size: 90\n",
            "100%|██████████| 100/100 [00:16<00:00,  5.91it/s, loss=0.0133]\n",
            "Accuracy = 0.7964\n",
            "=====\n",
            "Acquisition iteration 8 (26.67%), training size: 100\n",
            "100%|██████████| 100/100 [00:16<00:00,  5.90it/s, loss=0.00813]\n",
            "Accuracy = 0.8066\n",
            "=====\n",
            "Acquisition iteration 9 (30.00%), training size: 110\n",
            "100%|██████████| 100/100 [00:16<00:00,  5.88it/s, loss=0.0263]\n",
            "Accuracy = 0.8072\n",
            "=====\n",
            "Acquisition iteration 10 (33.33%), training size: 120\n",
            "100%|██████████| 100/100 [00:16<00:00,  5.90it/s, loss=0.0178]\n",
            "Accuracy = 0.8224\n",
            "=====\n",
            "Acquisition iteration 11 (36.67%), training size: 130\n",
            "100%|██████████| 100/100 [00:17<00:00,  5.64it/s, loss=0.0611]\n",
            "Accuracy = 0.8254\n",
            "=====\n",
            "Acquisition iteration 12 (40.00%), training size: 140\n",
            "100%|██████████| 100/100 [00:17<00:00,  5.61it/s, loss=0.0453]\n",
            "Accuracy = 0.8363\n",
            "=====\n",
            "Acquisition iteration 13 (43.33%), training size: 150\n",
            "100%|██████████| 100/100 [00:17<00:00,  5.59it/s, loss=0.0173]\n",
            "Accuracy = 0.8335\n",
            "=====\n",
            "Acquisition iteration 14 (46.67%), training size: 160\n",
            "100%|██████████| 100/100 [00:17<00:00,  5.60it/s, loss=0.0214]\n",
            "Accuracy = 0.8358\n",
            "=====\n",
            "Acquisition iteration 15 (50.00%), training size: 170\n",
            "100%|██████████| 100/100 [00:17<00:00,  5.58it/s, loss=0.0276]\n",
            "Accuracy = 0.8502\n",
            "=====\n",
            "Acquisition iteration 16 (53.33%), training size: 180\n",
            "100%|██████████| 100/100 [00:17<00:00,  5.59it/s, loss=0.0152]\n",
            "Accuracy = 0.8663\n",
            "=====\n",
            "Acquisition iteration 17 (56.67%), training size: 190\n",
            "100%|██████████| 100/100 [00:17<00:00,  5.59it/s, loss=0.0188]\n",
            "Accuracy = 0.8574\n",
            "=====\n",
            "Acquisition iteration 18 (60.00%), training size: 200\n",
            "100%|██████████| 100/100 [00:17<00:00,  5.58it/s, loss=0.0271]\n",
            "Accuracy = 0.8708\n",
            "=====\n",
            "Acquisition iteration 19 (63.33%), training size: 210\n",
            "100%|██████████| 100/100 [00:18<00:00,  5.53it/s, loss=0.0176]\n",
            "Accuracy = 0.8682\n",
            "=====\n",
            "Acquisition iteration 20 (66.67%), training size: 220\n",
            "100%|██████████| 100/100 [00:17<00:00,  5.57it/s, loss=0.0355]\n",
            "Accuracy = 0.871\n",
            "=====\n",
            "Acquisition iteration 21 (70.00%), training size: 230\n",
            "100%|██████████| 100/100 [00:18<00:00,  5.55it/s, loss=0.0204]\n",
            "Accuracy = 0.8658\n",
            "=====\n",
            "Acquisition iteration 22 (73.33%), training size: 240\n",
            "100%|██████████| 100/100 [00:17<00:00,  5.57it/s, loss=0.0167]\n",
            "Accuracy = 0.8714\n",
            "=====\n",
            "Acquisition iteration 23 (76.67%), training size: 250\n",
            "100%|██████████| 100/100 [00:17<00:00,  5.63it/s, loss=0.0206]\n",
            "Accuracy = 0.869\n",
            "=====\n",
            "Acquisition iteration 24 (80.00%), training size: 260\n",
            "100%|██████████| 100/100 [00:18<00:00,  5.54it/s, loss=0.0272]\n",
            "Accuracy = 0.8962\n",
            "=====\n",
            "Acquisition iteration 25 (83.33%), training size: 270\n",
            "100%|██████████| 100/100 [00:18<00:00,  5.41it/s, loss=0.0162]\n",
            "Accuracy = 0.8869\n",
            "=====\n",
            "Acquisition iteration 26 (86.67%), training size: 280\n",
            "100%|██████████| 100/100 [00:18<00:00,  5.40it/s, loss=0.0292]\n",
            "Accuracy = 0.8752\n",
            "=====\n",
            "Acquisition iteration 27 (90.00%), training size: 290\n",
            "100%|██████████| 100/100 [00:18<00:00,  5.41it/s, loss=0.018]\n",
            "Accuracy = 0.886\n",
            "=====\n",
            "Acquisition iteration 28 (93.33%), training size: 300\n",
            "100%|██████████| 100/100 [00:18<00:00,  5.46it/s, loss=0.0124]\n",
            "Accuracy = 0.8796\n",
            "=====\n",
            "Acquisition iteration 29 (96.67%), training size: 310\n",
            "100%|██████████| 100/100 [00:18<00:00,  5.39it/s, loss=0.0195]\n",
            "Accuracy = 0.8861\n",
            "=====\n",
            "Acquisition iteration 30 (100.00%), training size: 320\n",
            "100%|██████████| 100/100 [00:18<00:00,  5.34it/s, loss=0.0278]\n",
            "Accuracy = 0.8925\n",
            "=====\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UflY8EcQRt88",
        "colab_type": "code",
        "outputId": "5d51ab9c-58d8-4eae-e222-bad128532530",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plt.plot(list(history.keys()), list(history.values()), label='ICAL 10')\n",
        "plt.plot(list(ra_history.keys()), list(ra_history.values()), label='RA 10')\n",
        "plt.xlabel(\"Total training points\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.title(\"ICAL vs RA\");"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}