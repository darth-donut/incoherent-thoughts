{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pseudo-label: simple and efficient SSL method for DNNs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNO14Jwaa4T4w8AHzM3T5Kf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiahfong/incoherent-thoughts/blob/develop/Pseudo_label_simple_and_efficient_SSL_method_for_DNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJA_N6jXNwX5",
        "colab_type": "text"
      },
      "source": [
        "# Pseudo-Label: Simple and Efficient Semi-supervised learning method for deep NNs.\n",
        "\n",
        "## Executive summary\n",
        "\n",
        "1. Pseudo-labelling is in effect equivalent to Entropy Regularization. It favors a low-density separation between classes, a commonly assumed prior for semi-supervised learning. \n",
        "2. **Cluster assumption**: By minimizing the entropy for unlabeled data, the overlap of class probability distribution can be reduced (think: max-margin principle in SMVs). The *custer assumtion* states that the decision boundary should lie in low-density regions to improve generalization performance (Chapelle et al., 2005).\n",
        "3. The more general assumtion, **continuity assumtion** states that points nearby each other are likely to have the same class. A small change in input space is unlikely to change the target class.\n",
        "\n",
        "\n",
        "The algorithm goes as follows:\n",
        "\n",
        "for every epoch $e = 1 \\dots$ EPOCHS:\n",
        "1. calculate regular supervised loss (e.g. cross entropy)\n",
        "2. get pseudo-labels from unlabelled dataset (argmax softmax)\n",
        "3. total loss = supervised loss (CE) + $\\alpha(e)\\ *$ unlabelled loss (CE; using pseudo labels as \"true\" labels)\n",
        "\n",
        "$$\n",
        "\\alpha(e) = \n",
        "\\begin{cases} \n",
        "      0 & e\\le T_1 \\\\\n",
        "      \\frac{e - T_1}{T_2 - T_1}\\alpha_f & T_1 \\leq e \\lt T_2\\\\\n",
        "      \\alpha_f & T_2 \\leq e\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "$\\alpha_f = 3, T_1 = 100, T_2 = 600$ without pre-training and\n",
        "$T_1 = 200, T_2 = 800$ if used with denoising autoencoder as pre-training.\n",
        "\n",
        "4. Intuitively, what the unlabelled loss is doing is encouraging the model generalise. The unlabelled loss is large when the CE is large, which implies that the softmax distribution is evenly distributed amongst the classes. The model is forced to be decisive, that entails placing the decision boundary in areas where the class probability density is low (re. point 1, 2).\n",
        "\n",
        "\n",
        "# Closing remarks\n",
        "\n",
        "1. Perhaps trivially we might be able to deduce that the initial pre-training/training with labelled samples is extrememly important in this framework. If the initial labelled samples are too small for the model to predict useable pseudo-labels, then pseudo-labelling will probably make the model worse!"
      ]
    }
  ]
}